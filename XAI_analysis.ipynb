{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "from scipy import stats\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.ticker import MultipleLocator, FixedFormatter, FixedLocator\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "CSV_DIR = 'csv_merged'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Faithfulness Correlation',\n",
    "    'Faithfulness Estimate',\n",
    "    'Monotonicity Arya',\n",
    "    'Monotonicity Nguyen',\n",
    "    'Pixel-Flipping',\n",
    "    'Selectivity',\n",
    "    'Avg-Sensitivity',\n",
    "    'Local Lipschitz Estimate',\n",
    "    'Max-Sensitivity',\n",
    "    'Model Parameter Randomisation',\n",
    "    'Random Logit',\n",
    "    'Complexity',\n",
    "    'EffectiveComplexity',\n",
    "    'Sparseness'\n",
    "]\n",
    "                # ['SensitivityN': problem with implementation,\n",
    "                #'Region Perturbation' seems to be same as region perturbation, \n",
    "                #'Continuity Test': Difficult to aggregate result, the paper just plot it\n",
    "                #'Completeness' always returns False]\n",
    "                # Nonsentitivity is removed\n",
    "\n",
    "metrics_with_different_baselines = [\n",
    "    'Faithfulness Correlation',\n",
    "    'Faithfulness Estimate',\n",
    "    'Monotonicity Arya',\n",
    "    'Monotonicity Nguyen',\n",
    "    'Pixel-Flipping',\n",
    "    'Selectivity',\n",
    "]\n",
    "                \n",
    "baselines = [\n",
    "    'baseline_black',\n",
    "    # 'baseline_mean', not used anymore as there's some probs with quantus implementation\n",
    "    'baseline_random',\n",
    "    'baseline_uniform',\n",
    "    'baseline_white'\n",
    "]\n",
    "\n",
    "methods = [\n",
    "    'integratedgrad',\n",
    "    'smoothgrad',\n",
    "    'guidedbackprop',\n",
    "    'rise',\n",
    "    'gradcam',\n",
    "    'scorecam',\n",
    "    'layercam',\n",
    "    'random',\n",
    "    'sobel',\n",
    "    'gaussian',\n",
    "    'polycam',\n",
    "    'cameras'\n",
    "]\n",
    "#, 'extremal_perturbation']\n",
    "\n",
    "models = ['resnet50', 'vgg16']\n",
    "datasets = ['imagenet', 'cifar10']\n",
    "\n",
    "def is_done_model_with_dataset(model, dataset):\n",
    "    return model != 'vgg16' or dataset != 'cifar10'\n",
    "\n",
    "def is_done_metric_with_multiple_baselines(model, dataset, metric, method):\n",
    "    return (model == 'resnet50'\n",
    "            and dataset == 'imagenet'\n",
    "            and metric in metrics_with_different_baselines\n",
    "            and method != 'cameras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if recomputed metrics are done (i.e., if files exist)\n",
    "existing_files = set(os.listdir(CSV_DIR))\n",
    "expected_files = set()\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        if is_done_model_with_dataset(model, dataset):\n",
    "            for method in methods:\n",
    "                for metric in metrics:\n",
    "                    if is_done_metric_with_multiple_baselines(model, dataset, metric, method):\n",
    "                        for baseline in baselines:\n",
    "                            expected_files.add(f'{method}_{model}_{dataset}_{metric}_{baseline}.csv')\n",
    "                    else:\n",
    "                        expected_files.add(f'{method}_{model}_{dataset}_{metric}.csv')\n",
    "\n",
    "existing_files_but_not_expected = existing_files.difference(expected_files)\n",
    "expected_files_but_not_existing = expected_files.difference(existing_files)\n",
    "\n",
    "# print(f'EXISTING FILES BUT NOT EXPECTED: {len(existing_files_but_not_expected)}')\n",
    "# for f in sorted(existing_files_but_not_expected):\n",
    "#     print(f)\n",
    "# print()\n",
    "print(f'EXPECTED FILES BUT NOT EXISTING: {len(expected_files_but_not_existing)}')\n",
    "for f in sorted(expected_files_but_not_existing):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_method_dict(df, batch = 16):\n",
    "    ''''This function parse the dict and then compute the AUC per image\n",
    "        It returns a dataframe of the shape (Nb_Batch, Nb_images_per_bach)\n",
    "        np.trapz integrates the function under the curve\n",
    "    '''\n",
    "    dataf = pd.DataFrame(columns = [f\"{i}\" for i in range(batch)])\n",
    "    for i in range(df.shape[0]):\n",
    "        row = yaml.safe_load(df.iloc[i].iloc[0])\n",
    "        dataf.loc[i] = [np.trapz(row[j]) for j in row]\n",
    "    #print(dataf)\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version\n",
    "def parser_method_dict_with_layers(df, batch = 16):\n",
    "    ''''This function parse the dict and then compute the AUC per image\n",
    "        It returns a dataframe of the shape (Nb_Batch, Nb_images_per_bach)\n",
    "        np.trapz integrates the function under the curve\n",
    "        Note that df.iloc[0] has a dic e.g., {\"layer1\": [list of corr of bacth], \"layer2\": [list of corr of batch]}\n",
    "    '''\n",
    "    dataf = pd.DataFrame(columns = [f\"{i}\" for i in range(batch)])\n",
    "    for i in range(df.shape[0]): # loop over the number of batches\n",
    "        row = yaml.safe_load(df.iloc[i].iloc[0]) # Get the batch result\n",
    "        fillna = lambda v: 0.0 if v == 'nan' else v\n",
    "        # p is the index of image, j is the layer_name, np.trapz computes the AUC\n",
    "        dataf.loc[i] = [np.trapz([fillna(row[j][p]) for j in row]) for p in range(batch)]\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'Monotonicity Nguyen': lambda x: x,\n",
    "    'Local Lipschitz Estimate': lambda x: -x, \n",
    "    'Faithfulness Estimate': lambda x: x,\n",
    "    'Faithfulness Correlation': lambda x: x, \n",
    "    'Avg-Sensitivity': lambda x: -x,\n",
    "    'Random Logit': lambda x: x.fillna(0.),\n",
    "    'Sparseness': lambda x: x,\n",
    "    'EffectiveComplexity': lambda x: -x,\n",
    "    'Nonsensitivity': lambda x: -x,\n",
    "    'Pixel-Flipping': lambda x: x.apply(lambda row: - np.trapz(row), axis=1),\n",
    "    'Max-Sensitivity': lambda x: -x,\n",
    "    'Complexity': lambda x: -x, \n",
    "    'Selectivity': lambda x: -parser_method_dict(x),\n",
    "    'Model Parameter Randomisation': lambda x: -parser_method_dict_with_layers(x),\n",
    "    'Monotonicity Arya': lambda x: x,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores_dict = {}\n",
    "files = list(expected_files)\n",
    "errors = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    name = file[:-4]\n",
    "    metric = name.split('_')[3]\n",
    "    try:\n",
    "        df = pd.read_csv(f\"{CSV_DIR}/{file}\", header=None)\n",
    "        raw_scores_dict[name] = transform[metric](df).values.flatten()[:2000]\n",
    "    except:\n",
    "        errors.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics: \n",
    "    raw_scores_dict[transform[metric](df).values.flatten()[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in raw_scores_dict.items():\n",
    "    if len(data) != 2000:\n",
    "        print(f\"{key}: {len(data)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_scores_dict.pickle', 'wb') as file:\n",
    "    pickle.dump(raw_scores_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_scores_dict.pickle', 'rb') as file:\n",
    "    raw_scores_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(dataset, model, metric, method, baseline='baseline_black'):\n",
    "    results_key = f'{method}_{model}_{dataset}_{metric}_{baseline}'\n",
    "    alt_results_key = f'{method}_{model}_{dataset}_{metric}'\n",
    "    \n",
    "    if results_key in raw_scores_dict:\n",
    "        return raw_scores_dict[results_key]\n",
    "    else:\n",
    "        return raw_scores_dict[alt_results_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in raw_scores_dict.items():\n",
    "    if np.isnan(values).any():\n",
    "        print(name, np.count_nonzero(np.isnan(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build scores by metric lists\n",
    "scores_by_metric_dict = {}\n",
    "\n",
    "for model in models:\n",
    "    scores_by_metric_dict[model] = {}\n",
    "    for dataset in datasets:\n",
    "        if is_done_model_with_dataset(model, dataset):\n",
    "            scores_by_metric_dict[model][dataset] = {}\n",
    "            for metric in metrics:\n",
    "                print(metric)\n",
    "                results = [get_results(dataset, model, metric, method) for method in methods]\n",
    "                pivoted_results = list(map(list, zip(*results)))\n",
    "                scores_by_metric_dict[model][dataset][metric] = pivoted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with metrics with multiple baselines\n",
    "scores_by_metric_with_baselines_dict = {}\n",
    "model = 'resnet50'\n",
    "dataset = 'imagenet'\n",
    "done_methods = [method for method in methods if method != 'cameras'] # methods done with different baselines\n",
    "\n",
    "for metric in metrics_with_different_baselines:\n",
    "    for baseline in baselines:\n",
    "        results = [get_results(dataset, model, metric, method, baseline) for method in done_methods]\n",
    "        pivoted_results = list(map(list, zip(*results)))\n",
    "        scores_by_metric_with_baselines_dict[f'{metric}_{baseline}'] = pivoted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kendall_tau(scores_dict, metrics):\n",
    "    tau_values = []\n",
    "    p_values = []\n",
    "\n",
    "    for metric_a in metrics:\n",
    "        current_tau_values = []\n",
    "        current_p_values = []\n",
    "        for metric_b in metrics:\n",
    "            tau, p_value = stats.kendalltau(scores_dict[metric_a], scores_dict[metric_b])\n",
    "            current_tau_values.append(tau)\n",
    "            current_p_values.append(p_value)\n",
    "        tau_values.append(current_tau_values)\n",
    "        p_values.append(current_p_values)\n",
    "        \n",
    "    return tau_values, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr_matrix_by_mean_rank_aggreg(scores_dict, metrics):\n",
    "    aggregated_scores_dict = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        ranks = np.array(list(map(rankdata, scores_dict[metric])))\n",
    "        average_ranks = np.mean(ranks, axis=0)\n",
    "        aggregated_scores_dict[metric] = average_ranks\n",
    "    \n",
    "    return compute_kendall_tau(aggregated_scores_dict, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = {\n",
    "    'Monotonicity Nguyen': lambda x: np.tanh(np.nanmean(np.arctanh(x))),\n",
    "    'Local Lipschitz Estimate': np.nanmean,\n",
    "    'Faithfulness Estimate': lambda x: np.tanh(np.nanmean(np.arctanh(x))),\n",
    "    'Faithfulness Correlation': lambda x: np.tanh(np.nanmean(np.arctanh(x))),\n",
    "    'Avg-Sensitivity': np.nanmean,\n",
    "    'Random Logit': np.nanmean,\n",
    "    'Max-Sensitivity': np.nanmean,\n",
    "    'Sparseness': np.nanmean, \n",
    "    'EffectiveComplexity': np.nanmean,\n",
    "    'Monotonicity Arya': np.nanmean,\n",
    "    'Complexity': np.nanmean,\n",
    "    'Pixel-Flipping': np.nanmean,\n",
    "    'Selectivity': np.nanmean,\n",
    "    'Model Parameter Randomisation': np.nanmean\n",
    "}\n",
    "\n",
    "# def aggregate(metric_name):\n",
    "#     return lambda values: aggregate_fct_dict[metric_name]([v for v in values if not np.isnan(v)])\n",
    "\n",
    "def compute_corr_matrix_by_mean_score_aggreg(scores_dict, metrics):\n",
    "    aggregated_scores_dict = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        metric_name = metric.split('_')[0]\n",
    "        pivoted_scores = list(map(list, zip(*(scores_dict[metric]))))\n",
    "        average_scores = list(map(aggregate[metric_name], pivoted_scores))\n",
    "        aggregated_scores_dict[metric] = average_scores\n",
    "    \n",
    "    return compute_kendall_tau(aggregated_scores_dict, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr_matrix_by_mean_corr_aggreg(scores_dict, metrics):\n",
    "    tau_values_lists = []\n",
    "    p_values_lists = []\n",
    "    for i in tqdm(range(2000)):\n",
    "        subset_dict = { metric:scores_dict[metric][i] for metric in metrics }\n",
    "        tau_values, p_values = compute_kendall_tau(subset_dict, metrics)\n",
    "        tau_values_lists.append(tau_values)\n",
    "        p_values_lists.append(p_values)\n",
    "        \n",
    "    average_tau_values = np.mean(np.array(tau_values_lists), axis=0)\n",
    "    average_p_values = np.mean(np.array(p_values_lists), axis=0)\n",
    "    \n",
    "    return average_tau_values, average_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(tau_values, p_values, y_labels, x_labels, fig_size, filename=None,\n",
    "                     rotate_x=True, half_rotate_x=False, rotate_y=True, subgroups=None):\n",
    "    \n",
    "    p_values_flattened = [p_value for i, sublist in enumerate(p_values) for p_value in sublist[:i]]\n",
    "    reject, _, _, _ = multipletests(p_values_flattened, alpha=0.05, method='holm')\n",
    "    size = len(tau_values)\n",
    "\n",
    "    sn.set(rc={'figure.figsize': fig_size})\n",
    "\n",
    "    p_values_flattened = [p_value for i, sublist in enumerate(p_values) for p_value in sublist[:i]]\n",
    "    reject, _, _, _ = multipletests(p_values_flattened, alpha=0.05, method='holm')\n",
    "    mask = np.ones((size,size), dtype=bool)\n",
    "    current_post = 0\n",
    "    for i in range(size):\n",
    "        for j in range(i):\n",
    "            mask[i][j] = reject[current_post]\n",
    "            mask[j][i] = reject[current_post]\n",
    "            current_post += 1\n",
    "\n",
    "    sn.heatmap(tau_values,\n",
    "               annot=True,\n",
    "               vmin=-1,\n",
    "               vmax=1,\n",
    "               cbar=False,\n",
    "               xticklabels=x_labels,\n",
    "               yticklabels=y_labels,\n",
    "               mask=mask,\n",
    "               cmap='viridis')\n",
    "\n",
    "    mask = np.ones((size,size), dtype=bool)\n",
    "    current_post = 0\n",
    "    for i in range(size):\n",
    "        for j in range(i):\n",
    "            mask[i][j] = not reject[current_post]\n",
    "            mask[j][i] = not reject[current_post]\n",
    "            current_post += 1\n",
    "        mask[i][i] = False\n",
    "\n",
    "    ax = sn.heatmap(tau_values,\n",
    "                    annot=True,\n",
    "                    annot_kws={\"style\": \"italic\", \"weight\": \"bold\"},\n",
    "                    vmin=-1,\n",
    "                    vmax=1,\n",
    "                    cbar=False,\n",
    "                    xticklabels=x_labels,\n",
    "                    yticklabels=y_labels,\n",
    "                    mask=mask,\n",
    "                    cmap='viridis')\n",
    "\n",
    "    if half_rotate_x:\n",
    "        plt.xticks(rotation=20, ha=\"right\")\n",
    "    elif rotate_x:\n",
    "        plt.xticks(rotation=0)\n",
    "    \n",
    "    if rotate_y:\n",
    "        plt.yticks(rotation=0)\n",
    "    \n",
    "    start_i = 0\n",
    "    if subgroups:\n",
    "        for subgroup_size in subgroups:\n",
    "            ax.add_patch(Rectangle((start_i, start_i), subgroup_size, subgroup_size, fill=False, edgecolor='crimson', lw=4, clip_on=False))\n",
    "            start_i += subgroup_size\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(f'./results/{filename}.eps', bbox_inches='tight', format='eps')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures and results for article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness: same baseline and different metrics\n",
    "\n",
    "Only with ResNet-50 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_same_baseline_diff_metrics_figure(scores_dict, baseline):\n",
    "    selected_metrics = [f'{metric}_{baseline}' for metric in metrics_with_different_baselines]\n",
    "    short_labels = ['FC','FE', 'MA','MN','PF','Se']\n",
    "    filename = f'pakdd2023_corr_matrix_faithfulness_{baseline}'\n",
    "    tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_dict, selected_metrics)\n",
    "    plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                     fig_size=(5,3), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_baseline_diff_metrics_figure(scores_by_metric_with_baselines_dict, 'baseline_black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 / ImageNet (with cameras)\n",
    "short_labels = ['FC','FE', 'MA','MN','PF','Se']\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['imagenet'], metrics_with_different_baselines)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, fig_size=(5,3), rotate_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 / CIFAR-10 (with cameras)\n",
    "short_labels = ['FC','FE', 'MA','MN','PF','Se']\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['cifar10'], metrics_with_different_baselines)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, fig_size=(5,3), rotate_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 / ImageNet (with cameras)\n",
    "short_labels = ['FC','FE', 'MA','MN','PF','Se']\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['vgg16']['imagenet'], metrics_with_different_baselines)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, fig_size=(5,3), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_baseline_diff_metrics_figure(scores_by_metric_with_baselines_dict, 'baseline_random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_baseline_diff_metrics_figure(scores_by_metric_with_baselines_dict, 'baseline_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_baseline_diff_metrics_figure(scores_by_metric_with_baselines_dict, 'baseline_white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness: same metric and different baselines\n",
    "\n",
    "Only with ResNet-50 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_same_metric_diff_baselines_figure(scores_dict, metric):\n",
    "    selected_metrics = [f'{metric}_{baseline}' for baseline in baselines]\n",
    "    short_labels = ['B','R', 'U','W']\n",
    "    filename = f'pakdd2023_corr_matrix_{metric}'\n",
    "    tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_dict, selected_metrics)\n",
    "    plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                     fig_size=(3,2), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_metric_diff_baselines_figure(scores_by_metric_with_baselines_dict, 'Faithfulness Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_metric_diff_baselines_figure(scores_by_metric_with_baselines_dict, 'Faithfulness Estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonicity Arya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_metric_diff_baselines_figure(scores_by_metric_with_baselines_dict, 'Monotonicity Arya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonicity Nguyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_metric_diff_baselines_figure(scores_by_metric_with_baselines_dict, 'Monotonicity Nguyen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel-Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_metric_diff_baselines_figure(scores_by_metric_with_baselines_dict, 'Pixel-Flipping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_same_metric_diff_baselines_figure(scores_by_metric_with_baselines_dict, 'Selectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metrics = ['Complexity','EffectiveComplexity','Sparseness']\n",
    "labels = ['Complexity (C)', 'Effective Complexity (E)', 'Sparseness (S)']\n",
    "short_labels = ['C','E', 'S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_complexity_resnet50_imagenet'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['imagenet'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(2,1.5), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 / CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_complexity_resnet50_cifar10'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['cifar10'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(2,1.5), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_complexity_vgg16_imagenet'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['vgg16']['imagenet'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(2,1.5), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metrics = ['Model Parameter Randomisation', 'Random Logit']\n",
    "labels = ['Model Parameter Randomisation (M)', 'Random Logit (R)']\n",
    "short_labels = ['M','R']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_randomization_resnet50_imagenet'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['imagenet'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(1.5,1), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 / CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_randomization_resnet50_cifar10'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['cifar10'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(1.5,1), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_randomization_vgg16_imagenet'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['vgg16']['imagenet'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(1.5,1), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metrics = ['Avg-Sensitivity','Local Lipschitz Estimate', 'Max-Sensitivity']\n",
    "labels = ['Avg-Sensitivity (A)', 'Local Lipschitz Estimate (L)', 'Max-Sensitivity (M)']\n",
    "short_labels = ['A','L', 'M']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_robustness_resnet50_imagenet'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['imagenet'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(2,1.5), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 / CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_robustness_resnet50_cifar10'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['cifar10'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(2,1.5), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_robustness_vgg16_imagenet'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['vgg16']['imagenet'], selected_metrics)\n",
    "plot_corr_matrix(tau_values, p_values, short_labels, short_labels, filename=filename,\n",
    "                 fig_size=(2,1.5), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All metrics with default baselines (i.e. black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_all_metrics_resnet50_imagenet'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['imagenet'], metrics)\n",
    "plot_corr_matrix(tau_values, p_values, metrics, metrics, filename=filename,\n",
    "                 fig_size=(10,8), subgroups=(6,3,2,3), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-50 / CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_all_metrics_resnet50_cifar10'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['resnet50']['cifar10'], metrics)\n",
    "plot_corr_matrix(tau_values, p_values, metrics, metrics, filename=filename,\n",
    "                 fig_size=(10,8), subgroups=(6,3,2,3), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 / ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_corr_matrix_all_metrics_vgg16_imagenet'\n",
    "tau_values, p_values = compute_corr_matrix_by_mean_score_aggreg(scores_by_metric_dict['vgg16']['imagenet'], metrics)\n",
    "plot_corr_matrix(tau_values, p_values, metrics, metrics, filename=filename,\n",
    "                 fig_size=(10,8), subgroups=(6,3,2,3), rotate_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bump plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bump_plot(scores_dict, metrics, model_name, dataset_name, filename=None):\n",
    "    ranks_dict = {}\n",
    "\n",
    "    metrik = [\n",
    "    'Monotonicity Nguyen',\n",
    "    'Faithfulness Estimate',\n",
    "    'Faithfulness Correlation',\n",
    "    'Selectivity',\n",
    "    'Pixel-Flipping',\n",
    "    'Monotonicity Arya',\n",
    "    'Avg-Sensitivity',\n",
    "    'Max-Sensitivity',\n",
    "    'Local Lipschitz Estimate',\n",
    "    'Model Parameter Randomisation',\n",
    "    'Random Logit',\n",
    "    'EffectiveComplexity',\n",
    "    'Complexity',\n",
    "    'Sparseness'\n",
    "]\n",
    "    for metric in metrik:\n",
    "        scores = scores_dict[metric]\n",
    "        # Inverted scores are used so that rank 1 is the best rank\n",
    "        inverted_scores = [[-score for score in sublist] for sublist in scores]\n",
    "        ranks = np.array(list(map(rankdata, inverted_scores)))\n",
    "        average_ranks = np.mean(ranks, axis=0)\n",
    "        ranks_dict[metric] = average_ranks\n",
    "    \n",
    "    \n",
    "    dummy_methods = ['random', 'sobel', 'gaussian']\n",
    "\n",
    "    true_methods = ['integratedgrad','smoothgrad','guidedbackprop','rise','gradcam',\n",
    "                    'scorecam','layercam','polycam', 'cameras']\n",
    "\n",
    "    df_ranks = pd.DataFrame(ranks_dict, index=methods)\n",
    "\n",
    "    df_ranks_sorted = df_ranks[metrik]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(ylim=(0.5, len(methods) + 0.51)))\n",
    "\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "\n",
    "    yax2 = ax.secondary_yaxis(\"right\")\n",
    "    yax2.yaxis.set_major_locator(FixedLocator(df_ranks_sorted[df_ranks_sorted.columns[-1]].to_list()))\n",
    "    yax2.yaxis.set_major_formatter(FixedFormatter(df_ranks_sorted.index))\n",
    "\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "    transposed = df_ranks_sorted.transpose()\n",
    "\n",
    "    ax.plot(transposed[dummy_methods] ,\"o-\",mfc=\"w\", linewidth = 3)\n",
    "    ax.plot(transposed[true_methods] ,\"--\",mfc=\"w\", linewidth = 1)\n",
    "\n",
    "    plt.xticks(rotation=-15, ha='left')\n",
    "    ax.set(xlabel=\"Metric\", ylabel=\"Rank\",\n",
    "           title=f\"Method rank by metric ({model_name} / {dataset_name})\")\n",
    "    ax.grid(axis=\"x\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        plt.savefig(f'./results/{filename}.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_bump_plot_resnet50_imagenet'\n",
    "plot_bump_plot(scores_by_metric_dict['resnet50']['imagenet'], metrics,\n",
    "               'ResNet-50', 'ImageNet', filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_bump_plot_resnet50_cifar10'\n",
    "plot_bump_plot(scores_by_metric_dict['resnet50']['cifar10'], metrics,\n",
    "               'ResNet-50', 'CIFAR-10', filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pakdd2023_bump_plot_vgg16_imagenet'\n",
    "plot_bump_plot(scores_by_metric_dict['vgg16']['imagenet'], metrics,\n",
    "               'VGG-16', 'ImageNet', filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
